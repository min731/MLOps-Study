{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zj9chxjl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train_2</strong> at: <a href='https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification/runs/zj9chxjl' target=\"_blank\">https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification/runs/zj9chxjl</a><br/> View project at: <a href='https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification' target=\"_blank\">https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240622_151537-zj9chxjl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zj9chxjl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\LJM\\MLOps_Study\\Wandb\\wandb\\run-20240622_151715-tjbam7rq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification/runs/tjbam7rq' target=\"_blank\">Train_2</a></strong> to <a href='https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification' target=\"_blank\">https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification/runs/tjbam7rq' target=\"_blank\">https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification/runs/tjbam7rq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jmlim2/MLOps_Interview_CIFAR10_Classification/runs/tjbam7rq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x26e517b4f40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# !wandb login\n",
    "wandb.init(project='MLOps_Interview_CIFAR10_Classification',\n",
    "           name='Train_2'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "args = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size\n",
    "}\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download training data from open datasets.\n",
    "full_training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "train_size = int(0.8 * len(full_training_data))\n",
    "valid_size = len(full_training_data) - train_size\n",
    "training_data, validation_data = random_split(full_training_data, [train_size, valid_size])\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([16, 1, 28, 28])\n",
      "Shape of y: torch.Size([16]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=args['batch_size'])\n",
    "valid_dataloader = DataLoader(validation_data, batch_size=args['batch_size'])\n",
    "test_dataloader = DataLoader(test_data, batch_size=args['batch_size'])\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost function, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    wandb.log({\"train_loss\": total_loss / len(dataloader)}, step=epoch)\n",
    "\n",
    "def validate(dataloader, model, loss_fn, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    valid_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    valid_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "    wandb.log({\"valid_loss\": valid_loss, \"valid_acc\": correct}, step=epoch)\n",
    "    \n",
    "def test(dataloader, model, loss_fn, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    wandb.log({\"test_loss\": test_loss, \"test_acc\": correct}, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305412  [    0/48000]\n",
      "loss: 2.292319  [ 1600/48000]\n",
      "loss: 2.292901  [ 3200/48000]\n",
      "loss: 2.288993  [ 4800/48000]\n",
      "loss: 2.293443  [ 6400/48000]\n",
      "loss: 2.281941  [ 8000/48000]\n",
      "loss: 2.294272  [ 9600/48000]\n",
      "loss: 2.302605  [11200/48000]\n",
      "loss: 2.272305  [12800/48000]\n",
      "loss: 2.305001  [14400/48000]\n",
      "loss: 2.274620  [16000/48000]\n",
      "loss: 2.290270  [17600/48000]\n",
      "loss: 2.283370  [19200/48000]\n",
      "loss: 2.287082  [20800/48000]\n",
      "loss: 2.290796  [22400/48000]\n",
      "loss: 2.275784  [24000/48000]\n",
      "loss: 2.287327  [25600/48000]\n",
      "loss: 2.276525  [27200/48000]\n",
      "loss: 2.280521  [28800/48000]\n",
      "loss: 2.272375  [30400/48000]\n",
      "loss: 2.268008  [32000/48000]\n",
      "loss: 2.281464  [33600/48000]\n",
      "loss: 2.269574  [35200/48000]\n",
      "loss: 2.273771  [36800/48000]\n",
      "loss: 2.269223  [38400/48000]\n",
      "loss: 2.274923  [40000/48000]\n",
      "loss: 2.277888  [41600/48000]\n",
      "loss: 2.266937  [43200/48000]\n",
      "loss: 2.261793  [44800/48000]\n",
      "loss: 2.260916  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.259726 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.264802  [    0/48000]\n",
      "loss: 2.261073  [ 1600/48000]\n",
      "loss: 2.261772  [ 3200/48000]\n",
      "loss: 2.257102  [ 4800/48000]\n",
      "loss: 2.243079  [ 6400/48000]\n",
      "loss: 2.249426  [ 8000/48000]\n",
      "loss: 2.239857  [ 9600/48000]\n",
      "loss: 2.244575  [11200/48000]\n",
      "loss: 2.229513  [12800/48000]\n",
      "loss: 2.261572  [14400/48000]\n",
      "loss: 2.234078  [16000/48000]\n",
      "loss: 2.246927  [17600/48000]\n",
      "loss: 2.239270  [19200/48000]\n",
      "loss: 2.251133  [20800/48000]\n",
      "loss: 2.251038  [22400/48000]\n",
      "loss: 2.234847  [24000/48000]\n",
      "loss: 2.246573  [25600/48000]\n",
      "loss: 2.232676  [27200/48000]\n",
      "loss: 2.234761  [28800/48000]\n",
      "loss: 2.227261  [30400/48000]\n",
      "loss: 2.209839  [32000/48000]\n",
      "loss: 2.251894  [33600/48000]\n",
      "loss: 2.224524  [35200/48000]\n",
      "loss: 2.246632  [36800/48000]\n",
      "loss: 2.213128  [38400/48000]\n",
      "loss: 2.243387  [40000/48000]\n",
      "loss: 2.241341  [41600/48000]\n",
      "loss: 2.233313  [43200/48000]\n",
      "loss: 2.212889  [44800/48000]\n",
      "loss: 2.216751  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 47.6%, Avg loss: 2.215836 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.223828  [    0/48000]\n",
      "loss: 2.226971  [ 1600/48000]\n",
      "loss: 2.225928  [ 3200/48000]\n",
      "loss: 2.224133  [ 4800/48000]\n",
      "loss: 2.189506  [ 6400/48000]\n",
      "loss: 2.214573  [ 8000/48000]\n",
      "loss: 2.183988  [ 9600/48000]\n",
      "loss: 2.184889  [11200/48000]\n",
      "loss: 2.184832  [12800/48000]\n",
      "loss: 2.212219  [14400/48000]\n",
      "loss: 2.190317  [16000/48000]\n",
      "loss: 2.200622  [17600/48000]\n",
      "loss: 2.190009  [19200/48000]\n",
      "loss: 2.212562  [20800/48000]\n",
      "loss: 2.208283  [22400/48000]\n",
      "loss: 2.187416  [24000/48000]\n",
      "loss: 2.203189  [25600/48000]\n",
      "loss: 2.184254  [27200/48000]\n",
      "loss: 2.182977  [28800/48000]\n",
      "loss: 2.177102  [30400/48000]\n",
      "loss: 2.146171  [32000/48000]\n",
      "loss: 2.217026  [33600/48000]\n",
      "loss: 2.170688  [35200/48000]\n",
      "loss: 2.212107  [36800/48000]\n",
      "loss: 2.151139  [38400/48000]\n",
      "loss: 2.206117  [40000/48000]\n",
      "loss: 2.199612  [41600/48000]\n",
      "loss: 2.190984  [43200/48000]\n",
      "loss: 2.156054  [44800/48000]\n",
      "loss: 2.167317  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 2.163416 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.174338  [    0/48000]\n",
      "loss: 2.184160  [ 1600/48000]\n",
      "loss: 2.179948  [ 3200/48000]\n",
      "loss: 2.185630  [ 4800/48000]\n",
      "loss: 2.125458  [ 6400/48000]\n",
      "loss: 2.170141  [ 8000/48000]\n",
      "loss: 2.118863  [ 9600/48000]\n",
      "loss: 2.115244  [11200/48000]\n",
      "loss: 2.130995  [12800/48000]\n",
      "loss: 2.149227  [14400/48000]\n",
      "loss: 2.134919  [16000/48000]\n",
      "loss: 2.145407  [17600/48000]\n",
      "loss: 2.127932  [19200/48000]\n",
      "loss: 2.166490  [20800/48000]\n",
      "loss: 2.156936  [22400/48000]\n",
      "loss: 2.127605  [24000/48000]\n",
      "loss: 2.150593  [25600/48000]\n",
      "loss: 2.124449  [27200/48000]\n",
      "loss: 2.118190  [28800/48000]\n",
      "loss: 2.114009  [30400/48000]\n",
      "loss: 2.069098  [32000/48000]\n",
      "loss: 2.171687  [33600/48000]\n",
      "loss: 2.099919  [35200/48000]\n",
      "loss: 2.166179  [36800/48000]\n",
      "loss: 2.079202  [38400/48000]\n",
      "loss: 2.160093  [40000/48000]\n",
      "loss: 2.148410  [41600/48000]\n",
      "loss: 2.133899  [43200/48000]\n",
      "loss: 2.085193  [44800/48000]\n",
      "loss: 2.108145  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 2.096444 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.110228  [    0/48000]\n",
      "loss: 2.127778  [ 1600/48000]\n",
      "loss: 2.118496  [ 3200/48000]\n",
      "loss: 2.135756  [ 4800/48000]\n",
      "loss: 2.044151  [ 6400/48000]\n",
      "loss: 2.111535  [ 8000/48000]\n",
      "loss: 2.038451  [ 9600/48000]\n",
      "loss: 2.027658  [11200/48000]\n",
      "loss: 2.062653  [12800/48000]\n",
      "loss: 2.064679  [14400/48000]\n",
      "loss: 2.062872  [16000/48000]\n",
      "loss: 2.077921  [17600/48000]\n",
      "loss: 2.047892  [19200/48000]\n",
      "loss: 2.108167  [20800/48000]\n",
      "loss: 2.093180  [22400/48000]\n",
      "loss: 2.050489  [24000/48000]\n",
      "loss: 2.085308  [25600/48000]\n",
      "loss: 2.050288  [27200/48000]\n",
      "loss: 2.033741  [28800/48000]\n",
      "loss: 2.033920  [30400/48000]\n",
      "loss: 1.975009  [32000/48000]\n",
      "loss: 2.111513  [33600/48000]\n",
      "loss: 2.005444  [35200/48000]\n",
      "loss: 2.105621  [36800/48000]\n",
      "loss: 1.995167  [38400/48000]\n",
      "loss: 2.103216  [40000/48000]\n",
      "loss: 2.087100  [41600/48000]\n",
      "loss: 2.059176  [43200/48000]\n",
      "loss: 1.996889  [44800/48000]\n",
      "loss: 2.037041  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 2.010978 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.028139  [    0/48000]\n",
      "loss: 2.053893  [ 1600/48000]\n",
      "loss: 2.036863  [ 3200/48000]\n",
      "loss: 2.073611  [ 4800/48000]\n",
      "loss: 1.941483  [ 6400/48000]\n",
      "loss: 2.035251  [ 8000/48000]\n",
      "loss: 1.939343  [ 9600/48000]\n",
      "loss: 1.920039  [11200/48000]\n",
      "loss: 1.977773  [12800/48000]\n",
      "loss: 1.953798  [14400/48000]\n",
      "loss: 1.970645  [16000/48000]\n",
      "loss: 1.995093  [17600/48000]\n",
      "loss: 1.945800  [19200/48000]\n",
      "loss: 2.034261  [20800/48000]\n",
      "loss: 2.015763  [22400/48000]\n",
      "loss: 1.952407  [24000/48000]\n",
      "loss: 2.005633  [25600/48000]\n",
      "loss: 1.960024  [27200/48000]\n",
      "loss: 1.927840  [28800/48000]\n",
      "loss: 1.935653  [30400/48000]\n",
      "loss: 1.862878  [32000/48000]\n",
      "loss: 2.034435  [33600/48000]\n",
      "loss: 1.885358  [35200/48000]\n",
      "loss: 2.028562  [36800/48000]\n",
      "loss: 1.897214  [38400/48000]\n",
      "loss: 2.032994  [40000/48000]\n",
      "loss: 2.013912  [41600/48000]\n",
      "loss: 1.962588  [43200/48000]\n",
      "loss: 1.890708  [44800/48000]\n",
      "loss: 1.953953  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.905751 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.928929  [    0/48000]\n",
      "loss: 1.958555  [ 1600/48000]\n",
      "loss: 1.931353  [ 3200/48000]\n",
      "loss: 1.997880  [ 4800/48000]\n",
      "loss: 1.818447  [ 6400/48000]\n",
      "loss: 1.941646  [ 8000/48000]\n",
      "loss: 1.821364  [ 9600/48000]\n",
      "loss: 1.796093  [11200/48000]\n",
      "loss: 1.875076  [12800/48000]\n",
      "loss: 1.818033  [14400/48000]\n",
      "loss: 1.858140  [16000/48000]\n",
      "loss: 1.896738  [17600/48000]\n",
      "loss: 1.821203  [19200/48000]\n",
      "loss: 1.945432  [20800/48000]\n",
      "loss: 1.924121  [22400/48000]\n",
      "loss: 1.834565  [24000/48000]\n",
      "loss: 1.911339  [25600/48000]\n",
      "loss: 1.856145  [27200/48000]\n",
      "loss: 1.804698  [28800/48000]\n",
      "loss: 1.822419  [30400/48000]\n",
      "loss: 1.735247  [32000/48000]\n",
      "loss: 1.941552  [33600/48000]\n",
      "loss: 1.743470  [35200/48000]\n",
      "loss: 1.934514  [36800/48000]\n",
      "loss: 1.787516  [38400/48000]\n",
      "loss: 1.947654  [40000/48000]\n",
      "loss: 1.928779  [41600/48000]\n",
      "loss: 1.845656  [43200/48000]\n",
      "loss: 1.770784  [44800/48000]\n",
      "loss: 1.861316  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.785328 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.819780  [    0/48000]\n",
      "loss: 1.844284  [ 1600/48000]\n",
      "loss: 1.804002  [ 3200/48000]\n",
      "loss: 1.909028  [ 4800/48000]\n",
      "loss: 1.682735  [ 6400/48000]\n",
      "loss: 1.836934  [ 8000/48000]\n",
      "loss: 1.688231  [ 9600/48000]\n",
      "loss: 1.667693  [11200/48000]\n",
      "loss: 1.759710  [12800/48000]\n",
      "loss: 1.670840  [14400/48000]\n",
      "loss: 1.734111  [16000/48000]\n",
      "loss: 1.785161  [17600/48000]\n",
      "loss: 1.683182  [19200/48000]\n",
      "loss: 1.844958  [20800/48000]\n",
      "loss: 1.823641  [22400/48000]\n",
      "loss: 1.706890  [24000/48000]\n",
      "loss: 1.806010  [25600/48000]\n",
      "loss: 1.747766  [27200/48000]\n",
      "loss: 1.678948  [28800/48000]\n",
      "loss: 1.703463  [30400/48000]\n",
      "loss: 1.602009  [32000/48000]\n",
      "loss: 1.840013  [33600/48000]\n",
      "loss: 1.594322  [35200/48000]\n",
      "loss: 1.828089  [36800/48000]\n",
      "loss: 1.674220  [38400/48000]\n",
      "loss: 1.849462  [40000/48000]\n",
      "loss: 1.836167  [41600/48000]\n",
      "loss: 1.718540  [43200/48000]\n",
      "loss: 1.647166  [44800/48000]\n",
      "loss: 1.763782  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.660808 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.713549  [    0/48000]\n",
      "loss: 1.720172  [ 1600/48000]\n",
      "loss: 1.666145  [ 3200/48000]\n",
      "loss: 1.811595  [ 4800/48000]\n",
      "loss: 1.547553  [ 6400/48000]\n",
      "loss: 1.730594  [ 8000/48000]\n",
      "loss: 1.550128  [ 9600/48000]\n",
      "loss: 1.555112  [11200/48000]\n",
      "loss: 1.641147  [12800/48000]\n",
      "loss: 1.532624  [14400/48000]\n",
      "loss: 1.610995  [16000/48000]\n",
      "loss: 1.668643  [17600/48000]\n",
      "loss: 1.545997  [19200/48000]\n",
      "loss: 1.742016  [20800/48000]\n",
      "loss: 1.721521  [22400/48000]\n",
      "loss: 1.583629  [24000/48000]\n",
      "loss: 1.697441  [25600/48000]\n",
      "loss: 1.645887  [27200/48000]\n",
      "loss: 1.567184  [28800/48000]\n",
      "loss: 1.590943  [30400/48000]\n",
      "loss: 1.476099  [32000/48000]\n",
      "loss: 1.740057  [33600/48000]\n",
      "loss: 1.454816  [35200/48000]\n",
      "loss: 1.718618  [36800/48000]\n",
      "loss: 1.565413  [38400/48000]\n",
      "loss: 1.744868  [40000/48000]\n",
      "loss: 1.743411  [41600/48000]\n",
      "loss: 1.592735  [43200/48000]\n",
      "loss: 1.529539  [44800/48000]\n",
      "loss: 1.668219  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.543734 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.622729  [    0/48000]\n",
      "loss: 1.595828  [ 1600/48000]\n",
      "loss: 1.531688  [ 3200/48000]\n",
      "loss: 1.713946  [ 4800/48000]\n",
      "loss: 1.424062  [ 6400/48000]\n",
      "loss: 1.630335  [ 8000/48000]\n",
      "loss: 1.418259  [ 9600/48000]\n",
      "loss: 1.468468  [11200/48000]\n",
      "loss: 1.528534  [12800/48000]\n",
      "loss: 1.415992  [14400/48000]\n",
      "loss: 1.500336  [16000/48000]\n",
      "loss: 1.557380  [17600/48000]\n",
      "loss: 1.420079  [19200/48000]\n",
      "loss: 1.646171  [20800/48000]\n",
      "loss: 1.625080  [22400/48000]\n",
      "loss: 1.472992  [24000/48000]\n",
      "loss: 1.593576  [25600/48000]\n",
      "loss: 1.558215  [27200/48000]\n",
      "loss: 1.476307  [28800/48000]\n",
      "loss: 1.489876  [30400/48000]\n",
      "loss: 1.365211  [32000/48000]\n",
      "loss: 1.648539  [33600/48000]\n",
      "loss: 1.332804  [35200/48000]\n",
      "loss: 1.614655  [36800/48000]\n",
      "loss: 1.465587  [38400/48000]\n",
      "loss: 1.640525  [40000/48000]\n",
      "loss: 1.658152  [41600/48000]\n",
      "loss: 1.475302  [43200/48000]\n",
      "loss: 1.422654  [44800/48000]\n",
      "loss: 1.580460  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.440235 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.552328  [    0/48000]\n",
      "loss: 1.477859  [ 1600/48000]\n",
      "loss: 1.410681  [ 3200/48000]\n",
      "loss: 1.624631  [ 4800/48000]\n",
      "loss: 1.316320  [ 6400/48000]\n",
      "loss: 1.539030  [ 8000/48000]\n",
      "loss: 1.299607  [ 9600/48000]\n",
      "loss: 1.402939  [11200/48000]\n",
      "loss: 1.428043  [12800/48000]\n",
      "loss: 1.321251  [14400/48000]\n",
      "loss: 1.405819  [16000/48000]\n",
      "loss: 1.458548  [17600/48000]\n",
      "loss: 1.309131  [19200/48000]\n",
      "loss: 1.564002  [20800/48000]\n",
      "loss: 1.538320  [22400/48000]\n",
      "loss: 1.378103  [24000/48000]\n",
      "loss: 1.500414  [25600/48000]\n",
      "loss: 1.485884  [27200/48000]\n",
      "loss: 1.404587  [28800/48000]\n",
      "loss: 1.400933  [30400/48000]\n",
      "loss: 1.271449  [32000/48000]\n",
      "loss: 1.567863  [33600/48000]\n",
      "loss: 1.229803  [35200/48000]\n",
      "loss: 1.521255  [36800/48000]\n",
      "loss: 1.375752  [38400/48000]\n",
      "loss: 1.542938  [40000/48000]\n",
      "loss: 1.584972  [41600/48000]\n",
      "loss: 1.369296  [43200/48000]\n",
      "loss: 1.327856  [44800/48000]\n",
      "loss: 1.503522  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.351437 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.501807  [    0/48000]\n",
      "loss: 1.370733  [ 1600/48000]\n",
      "loss: 1.306598  [ 3200/48000]\n",
      "loss: 1.547706  [ 4800/48000]\n",
      "loss: 1.223192  [ 6400/48000]\n",
      "loss: 1.457996  [ 8000/48000]\n",
      "loss: 1.195618  [ 9600/48000]\n",
      "loss: 1.350603  [11200/48000]\n",
      "loss: 1.341757  [12800/48000]\n",
      "loss: 1.243953  [14400/48000]\n",
      "loss: 1.326946  [16000/48000]\n",
      "loss: 1.374392  [17600/48000]\n",
      "loss: 1.212712  [19200/48000]\n",
      "loss: 1.497105  [20800/48000]\n",
      "loss: 1.460763  [22400/48000]\n",
      "loss: 1.297756  [24000/48000]\n",
      "loss: 1.419744  [25600/48000]\n",
      "loss: 1.426670  [27200/48000]\n",
      "loss: 1.348140  [28800/48000]\n",
      "loss: 1.322740  [30400/48000]\n",
      "loss: 1.193246  [32000/48000]\n",
      "loss: 1.498256  [33600/48000]\n",
      "loss: 1.144161  [35200/48000]\n",
      "loss: 1.439488  [36800/48000]\n",
      "loss: 1.295468  [38400/48000]\n",
      "loss: 1.455585  [40000/48000]\n",
      "loss: 1.524463  [41600/48000]\n",
      "loss: 1.275802  [43200/48000]\n",
      "loss: 1.244501  [44800/48000]\n",
      "loss: 1.437482  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.275901 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.466161  [    0/48000]\n",
      "loss: 1.275763  [ 1600/48000]\n",
      "loss: 1.218250  [ 3200/48000]\n",
      "loss: 1.482916  [ 4800/48000]\n",
      "loss: 1.142347  [ 6400/48000]\n",
      "loss: 1.387048  [ 8000/48000]\n",
      "loss: 1.106239  [ 9600/48000]\n",
      "loss: 1.305556  [11200/48000]\n",
      "loss: 1.268488  [12800/48000]\n",
      "loss: 1.179265  [14400/48000]\n",
      "loss: 1.261283  [16000/48000]\n",
      "loss: 1.303532  [17600/48000]\n",
      "loss: 1.128670  [19200/48000]\n",
      "loss: 1.443558  [20800/48000]\n",
      "loss: 1.391072  [22400/48000]\n",
      "loss: 1.229592  [24000/48000]\n",
      "loss: 1.349792  [25600/48000]\n",
      "loss: 1.377859  [27200/48000]\n",
      "loss: 1.302445  [28800/48000]\n",
      "loss: 1.254018  [30400/48000]\n",
      "loss: 1.128515  [32000/48000]\n",
      "loss: 1.437966  [33600/48000]\n",
      "loss: 1.072528  [35200/48000]\n",
      "loss: 1.367847  [36800/48000]\n",
      "loss: 1.223610  [38400/48000]\n",
      "loss: 1.379495  [40000/48000]\n",
      "loss: 1.474934  [41600/48000]\n",
      "loss: 1.194117  [43200/48000]\n",
      "loss: 1.171897  [44800/48000]\n",
      "loss: 1.381412  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.211395 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.440705  [    0/48000]\n",
      "loss: 1.192245  [ 1600/48000]\n",
      "loss: 1.143444  [ 3200/48000]\n",
      "loss: 1.427953  [ 4800/48000]\n",
      "loss: 1.071493  [ 6400/48000]\n",
      "loss: 1.325219  [ 8000/48000]\n",
      "loss: 1.029538  [ 9600/48000]\n",
      "loss: 1.264978  [11200/48000]\n",
      "loss: 1.206110  [12800/48000]\n",
      "loss: 1.124089  [14400/48000]\n",
      "loss: 1.205919  [16000/48000]\n",
      "loss: 1.243565  [17600/48000]\n",
      "loss: 1.055051  [19200/48000]\n",
      "loss: 1.399920  [20800/48000]\n",
      "loss: 1.328454  [22400/48000]\n",
      "loss: 1.171478  [24000/48000]\n",
      "loss: 1.287869  [25600/48000]\n",
      "loss: 1.336954  [27200/48000]\n",
      "loss: 1.264635  [28800/48000]\n",
      "loss: 1.193585  [30400/48000]\n",
      "loss: 1.075105  [32000/48000]\n",
      "loss: 1.385343  [33600/48000]\n",
      "loss: 1.011863  [35200/48000]\n",
      "loss: 1.304677  [36800/48000]\n",
      "loss: 1.159519  [38400/48000]\n",
      "loss: 1.313823  [40000/48000]\n",
      "loss: 1.433801  [41600/48000]\n",
      "loss: 1.122733  [43200/48000]\n",
      "loss: 1.108856  [44800/48000]\n",
      "loss: 1.333728  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.155911 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.421872  [    0/48000]\n",
      "loss: 1.119421  [ 1600/48000]\n",
      "loss: 1.079734  [ 3200/48000]\n",
      "loss: 1.381063  [ 4800/48000]\n",
      "loss: 1.008565  [ 6400/48000]\n",
      "loss: 1.271511  [ 8000/48000]\n",
      "loss: 0.963956  [ 9600/48000]\n",
      "loss: 1.227807  [11200/48000]\n",
      "loss: 1.152668  [12800/48000]\n",
      "loss: 1.076057  [14400/48000]\n",
      "loss: 1.158652  [16000/48000]\n",
      "loss: 1.192054  [17600/48000]\n",
      "loss: 0.990242  [19200/48000]\n",
      "loss: 1.363271  [20800/48000]\n",
      "loss: 1.271744  [22400/48000]\n",
      "loss: 1.121264  [24000/48000]\n",
      "loss: 1.233039  [25600/48000]\n",
      "loss: 1.302268  [27200/48000]\n",
      "loss: 1.233379  [28800/48000]\n",
      "loss: 1.140462  [30400/48000]\n",
      "loss: 1.031374  [32000/48000]\n",
      "loss: 1.338580  [33600/48000]\n",
      "loss: 0.960428  [35200/48000]\n",
      "loss: 1.248305  [36800/48000]\n",
      "loss: 1.102451  [38400/48000]\n",
      "loss: 1.257416  [40000/48000]\n",
      "loss: 1.399459  [41600/48000]\n",
      "loss: 1.060646  [43200/48000]\n",
      "loss: 1.054214  [44800/48000]\n",
      "loss: 1.293033  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.107962 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.407735  [    0/48000]\n",
      "loss: 1.055757  [ 1600/48000]\n",
      "loss: 1.025444  [ 3200/48000]\n",
      "loss: 1.340491  [ 4800/48000]\n",
      "loss: 0.952671  [ 6400/48000]\n",
      "loss: 1.224775  [ 8000/48000]\n",
      "loss: 0.907775  [ 9600/48000]\n",
      "loss: 1.193602  [11200/48000]\n",
      "loss: 1.106460  [12800/48000]\n",
      "loss: 1.033684  [14400/48000]\n",
      "loss: 1.117795  [16000/48000]\n",
      "loss: 1.147829  [17600/48000]\n",
      "loss: 0.933122  [19200/48000]\n",
      "loss: 1.332625  [20800/48000]\n",
      "loss: 1.219865  [22400/48000]\n",
      "loss: 1.077798  [24000/48000]\n",
      "loss: 1.184151  [25600/48000]\n",
      "loss: 1.272591  [27200/48000]\n",
      "loss: 1.207031  [28800/48000]\n",
      "loss: 1.093662  [30400/48000]\n",
      "loss: 0.995254  [32000/48000]\n",
      "loss: 1.296740  [33600/48000]\n",
      "loss: 0.916863  [35200/48000]\n",
      "loss: 1.197912  [36800/48000]\n",
      "loss: 1.051583  [38400/48000]\n",
      "loss: 1.209026  [40000/48000]\n",
      "loss: 1.370261  [41600/48000]\n",
      "loss: 1.006369  [43200/48000]\n",
      "loss: 1.006853  [44800/48000]\n",
      "loss: 1.258326  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.066350 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.397093  [    0/48000]\n",
      "loss: 1.000042  [ 1600/48000]\n",
      "loss: 0.978990  [ 3200/48000]\n",
      "loss: 1.305073  [ 4800/48000]\n",
      "loss: 0.902946  [ 6400/48000]\n",
      "loss: 1.183873  [ 8000/48000]\n",
      "loss: 0.859633  [ 9600/48000]\n",
      "loss: 1.161908  [11200/48000]\n",
      "loss: 1.066260  [12800/48000]\n",
      "loss: 0.995688  [14400/48000]\n",
      "loss: 1.081960  [16000/48000]\n",
      "loss: 1.109757  [17600/48000]\n",
      "loss: 0.882770  [19200/48000]\n",
      "loss: 1.306548  [20800/48000]\n",
      "loss: 1.172215  [22400/48000]\n",
      "loss: 1.040035  [24000/48000]\n",
      "loss: 1.140328  [25600/48000]\n",
      "loss: 1.247041  [27200/48000]\n",
      "loss: 1.184591  [28800/48000]\n",
      "loss: 1.052145  [30400/48000]\n",
      "loss: 0.965350  [32000/48000]\n",
      "loss: 1.259011  [33600/48000]\n",
      "loss: 0.879841  [35200/48000]\n",
      "loss: 1.152550  [36800/48000]\n",
      "loss: 1.006239  [38400/48000]\n",
      "loss: 1.167228  [40000/48000]\n",
      "loss: 1.345074  [41600/48000]\n",
      "loss: 0.958547  [43200/48000]\n",
      "loss: 0.965676  [44800/48000]\n",
      "loss: 1.228464  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.029984 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.389216  [    0/48000]\n",
      "loss: 0.951030  [ 1600/48000]\n",
      "loss: 0.938997  [ 3200/48000]\n",
      "loss: 1.273627  [ 4800/48000]\n",
      "loss: 0.858483  [ 6400/48000]\n",
      "loss: 1.148011  [ 8000/48000]\n",
      "loss: 0.818047  [ 9600/48000]\n",
      "loss: 1.132391  [11200/48000]\n",
      "loss: 1.031093  [12800/48000]\n",
      "loss: 0.961324  [14400/48000]\n",
      "loss: 1.050322  [16000/48000]\n",
      "loss: 1.076727  [17600/48000]\n",
      "loss: 0.838114  [19200/48000]\n",
      "loss: 1.283842  [20800/48000]\n",
      "loss: 1.128154  [22400/48000]\n",
      "loss: 1.007227  [24000/48000]\n",
      "loss: 1.100968  [25600/48000]\n",
      "loss: 1.224925  [27200/48000]\n",
      "loss: 1.165008  [28800/48000]\n",
      "loss: 1.015388  [30400/48000]\n",
      "loss: 0.940408  [32000/48000]\n",
      "loss: 1.224681  [33600/48000]\n",
      "loss: 0.848107  [35200/48000]\n",
      "loss: 1.111488  [36800/48000]\n",
      "loss: 0.965777  [38400/48000]\n",
      "loss: 1.130892  [40000/48000]\n",
      "loss: 1.322767  [41600/48000]\n",
      "loss: 0.916089  [43200/48000]\n",
      "loss: 0.929903  [44800/48000]\n",
      "loss: 1.202612  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.998035 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.383391  [    0/48000]\n",
      "loss: 0.907723  [ 1600/48000]\n",
      "loss: 0.904418  [ 3200/48000]\n",
      "loss: 1.245540  [ 4800/48000]\n",
      "loss: 0.818696  [ 6400/48000]\n",
      "loss: 1.116262  [ 8000/48000]\n",
      "loss: 0.782057  [ 9600/48000]\n",
      "loss: 1.105028  [11200/48000]\n",
      "loss: 1.000054  [12800/48000]\n",
      "loss: 0.929639  [14400/48000]\n",
      "loss: 1.022208  [16000/48000]\n",
      "loss: 1.048075  [17600/48000]\n",
      "loss: 0.798494  [19200/48000]\n",
      "loss: 1.263802  [20800/48000]\n",
      "loss: 1.087402  [22400/48000]\n",
      "loss: 0.978638  [24000/48000]\n",
      "loss: 1.065563  [25600/48000]\n",
      "loss: 1.205465  [27200/48000]\n",
      "loss: 1.147482  [28800/48000]\n",
      "loss: 0.982578  [30400/48000]\n",
      "loss: 0.919397  [32000/48000]\n",
      "loss: 1.193269  [33600/48000]\n",
      "loss: 0.820871  [35200/48000]\n",
      "loss: 1.074287  [36800/48000]\n",
      "loss: 0.929644  [38400/48000]\n",
      "loss: 1.099202  [40000/48000]\n",
      "loss: 1.302640  [41600/48000]\n",
      "loss: 0.878112  [43200/48000]\n",
      "loss: 0.898676  [44800/48000]\n",
      "loss: 1.180313  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.969804 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.378950  [    0/48000]\n",
      "loss: 0.869306  [ 1600/48000]\n",
      "loss: 0.874292  [ 3200/48000]\n",
      "loss: 1.220260  [ 4800/48000]\n",
      "loss: 0.782879  [ 6400/48000]\n",
      "loss: 1.087850  [ 8000/48000]\n",
      "loss: 0.750634  [ 9600/48000]\n",
      "loss: 1.079496  [11200/48000]\n",
      "loss: 0.972506  [12800/48000]\n",
      "loss: 0.900263  [14400/48000]\n",
      "loss: 0.997035  [16000/48000]\n",
      "loss: 1.022877  [17600/48000]\n",
      "loss: 0.763283  [19200/48000]\n",
      "loss: 1.245875  [20800/48000]\n",
      "loss: 1.049589  [22400/48000]\n",
      "loss: 0.953684  [24000/48000]\n",
      "loss: 1.033861  [25600/48000]\n",
      "loss: 1.188187  [27200/48000]\n",
      "loss: 1.131232  [28800/48000]\n",
      "loss: 0.953157  [30400/48000]\n",
      "loss: 0.901418  [32000/48000]\n",
      "loss: 1.164391  [33600/48000]\n",
      "loss: 0.797578  [35200/48000]\n",
      "loss: 1.040531  [36800/48000]\n",
      "loss: 0.897285  [38400/48000]\n",
      "loss: 1.071409  [40000/48000]\n",
      "loss: 1.284303  [41600/48000]\n",
      "loss: 0.844001  [43200/48000]\n",
      "loss: 0.871313  [44800/48000]\n",
      "loss: 1.160828  [46400/48000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.944711 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = args['epochs']\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, t)\n",
    "    validate(valid_dataloader, model, loss_fn, t)\n",
    "    test(test_dataloader, model, loss_fn, t)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = './models/'\n",
    "\n",
    "torch.save(model.state_dict(), save_dir + \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(save_dir + \"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
